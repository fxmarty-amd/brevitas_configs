# non-defaults only; see default_template.yml for defaults
# adds a BOS token to the beginning of each sequence
bos_preprocessing: sequence
dtype: bfloat16
eval: true
few_shot_eval: lm_eval
few_shot_tasks:
- leaderboard_mmlu_pro
- leaderboard_gpqa_main
- piqa
- arc_easy
- arc_challenge
- hellaswag
- winogrande
- agieval_en
few_shot_zeroshot: true
# enables ordering by diagonal of H
gpxq_act_order: true
gpxq_block_name: model.layers
# enables OCP MXFP4 activation quantization
input_bit_width: 4
input_group_size: 32
input_param_method: stats
input_quant_format: float_ocp_e2m1
input_quant_granularity: per_group
input_quant_type: sym
input_scale_precision: po2_scale
input_scale_type: dynamic
model: meta-llama/Llama-3.2-1B-Instruct
scale_rounding_func_type: round
# enables OCP MXFP4 weight quantization
weight_bit_width: 4
weight_group_size: 32
weight_param_method: stats
weight_quant_format: float_ocp_e2m1
weight_quant_granularity: per_group
weight_quant_type: sym
weight_scale_precision: po2_scale
# enables Qronos
qronos: true
qronos_alpha: 1e-3
# Save params
# checkpoint_name: "checkpoints/llama3.2_1b_instruct_w4a4_mxfp4_qronos.pt"